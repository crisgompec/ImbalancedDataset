%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Cristian  at 2020-11-28 23:38:16 -0500 


%% Saved with string encoding Unicode (UTF-8) 



@article{improved_boosting,
	abstract = {We describe several improvements to Freund and Schapire's AdaBoost boosting algorithm, particularly in a setting in which hypotheses may assign confidences to each of their predictions. We give a simplified analysis of AdaBoost in this setting, and we show how this analysis can be used to find improved parameter settings as well as a refined criterion for training weak hypotheses. We give a specific method for assigning confidences to the predictions of decision trees, a method closely related to one used by Quinlan. This method also suggests a technique for growing decision trees which turns out to be identical to one proposed by Kearns and Mansour. We focus next on how to apply the new boosting algorithms to multiclass classification problems, particularly to the multi-label case in which each example may belong to more than one class. We give two boosting methods for this problem, plus a third method based on output coding. One of these leads to a new method for handling the single-label case which is simpler but as effective as techniques suggested by Freund and Schapire. Finally, we give some experimental results comparing a few of the algorithms discussed in this paper.},
	author = {Schapire, {Robert E.} and Yoram Singer},
	date-added = {2020-11-28 23:36:59 -0500},
	date-modified = {2020-11-28 23:36:59 -0500},
	doi = {10.1023/A:1007614523901},
	issn = {0885-6125},
	journal = {Machine Learning},
	language = {English (US)},
	month = dec,
	note = {Proceedings of the 1998 11th Annual Conference on 'Computational Learning Theory' (COLT98) ; Conference date: 24-07-1998 Through 26-07-1998},
	number = {3},
	pages = {297--336},
	publisher = {Springer Netherlands},
	title = {Improved boosting algorithms using confidence-rated predictions},
	volume = {37},
	year = {1999},
	Bdsk-Url-1 = {https://doi.org/10.1023/A:1007614523901}}

@article{adacost,
	author = {Fan, Wei and Stolfo, Salvatore and Zhang, Junxin and Chan, Philip},
	date-added = {2020-11-28 23:34:56 -0500},
	date-modified = {2020-11-28 23:34:56 -0500},
	journal = {Proceedings of the Sixteenth International Conference on Machine Learning (ICML'99)},
	month = {05},
	title = {AdaCost: Misclassification Cost-sensitive Boosting},
	year = {1999}}

@inproceedings{boosting_svm,
	author = {Elkin Garc{\'{\i}}a and Fernando Lozano},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/mldm/GarciaL07.bib},
	booktitle = {Machine Learning and Data Mining in Pattern Recognition, 5th International Conference, {MLDM} 2007, Leipzig, Germany, July 18-20, 2007, Post Proceedings},
	date-added = {2020-11-28 23:29:44 -0500},
	date-modified = {2020-11-28 23:29:44 -0500},
	editor = {Petra Perner},
	pages = {153--167},
	publisher = {IBaI publishing},
	timestamp = {Tue, 01 Sep 2009 10:00:57 +0200},
	title = {Boosting Support Vector Machines},
	year = {2007}}

@inproceedings{study_boosted_svm,
	abstract = {In this article, we focus on designing an algorithm, named AdaBoostSVM, using SVM as weak learners for AdaBoost. To obtain a set of effective SVM weak learners, this algorithm adaptively adjusts the kernel parameter in SVM instead of using a fixed one. Compared with the existing AdaBoost methods, the AdaBoostSVM has advantages of easier model selection and better generalization performance. It also provides a possible way to handle the over-fitting problem in AdaBoost. An improved version called Diverse AdaBoostSVM is further developed to deal with the accuracy/diversity dilemma in Boosting methods. By implementing some parameter adjusting strategies, the distributions of accuracy and diversity over these SVM weak learners are tuned to achieve a good balance. To the best of our knowledge, such a mechanism that can conveniently and explicitly balances this dilemma has not been seen in the literature. Experimental results demonstrated that both proposed algorithms achieve better generalization performance than AdaBoost using other kinds of weak learners. Benefiting from the balance between accuracy and diversity, the Diverse AdaBoostSVM achieves the best performance. In addition, the experiments on unbalanced data sets showed that the AdaBoostSVM performed much better than SVM.},
	author = {{Xuchun Li} and {Lei Wang} and E. {Sung}},
	booktitle = {Proceedings. 2005 IEEE International Joint Conference on Neural Networks, 2005.},
	date-added = {2020-11-28 23:23:59 -0500},
	date-modified = {2020-11-28 23:23:59 -0500},
	doi = {10.1109/IJCNN.2005.1555829},
	issn = {2161-4407},
	keywords = {adaptive systems;learning (artificial intelligence);support vector machines;SVM weak learners;kernel parameter;AdaBoost method;model selection;over-fitting problem;Diverse AdaBoostSVM;Support vector machines;Kernel;Boosting;Diversity reception;Bagging;Decision trees;Neural networks;Design engineering;Algorithm design and analysis;Support vector machine classification},
	month = {July},
	pages = {196-201 vol. 1},
	title = {A study of AdaBoost with SVM based weak learners},
	volume = {1},
	year = {2005},
	Bdsk-Url-1 = {https://doi.org/10.1109/IJCNN.2005.1555829}}

@article{need_boosting,
	author = {Nikolaou, Nikos and Edakunni, Narayanan and Kull, Meelis and Flach, Peter and Brown, Gavin},
	date-added = {2020-11-28 23:20:56 -0500},
	date-modified = {2020-11-28 23:20:56 -0500},
	doi = {10.1007/s10994-016-5572-x},
	journal = {Machine Learning},
	month = {09},
	title = {Cost-sensitive boosting algorithms: Do we really need them?},
	volume = {104},
	year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1007/s10994-016-5572-x}}

@article{margin_adaboost,
	author = {Rudin, Cynthia and Daubechies, Ingrid and Schapire, Robert},
	date-added = {2020-11-28 23:18:34 -0500},
	date-modified = {2020-11-28 23:18:34 -0500},
	journal = {Journal of Machine Learning Research},
	month = {12},
	pages = {1557-1595},
	title = {The Dynamics of AdaBoost: Cyclic Behavior and Convergence of Margins},
	volume = {5},
	year = {2004}}

@article{BoostingStats,
	author = {Jerome Friedman and Trevor Hastie and Robert Tibshirani},
	date-added = {2020-11-28 23:06:16 -0500},
	date-modified = {2020-11-28 23:06:16 -0500},
	journal = {Annals of Statistics},
	pages = {2000},
	title = {Additive Logistic Regression: a Statistical View of Boosting},
	volume = {28},
	year = {1998}}

@article{top10algo,
	author = {Wu, Xindong and Kumar, Vipin and Quinlan, Ross and Ghosh, Joydeep and Yang, Qiang and Motoda, Hiroshi and Mclachlan, G. and Ng, Shu Kay Angus and Liu, Bing and Yu, Philip and Zhou, Zhi-Hua and Steinbach, Michael and Hand, David and Steinberg, Dan},
	date-added = {2020-11-28 23:02:43 -0500},
	date-modified = {2020-11-28 23:02:43 -0500},
	doi = {10.1007/s10115-007-0114-2},
	journal = {Knowledge and Information Systems},
	month = {12},
	title = {Top 10 algorithms in data mining},
	volume = {14},
	year = {2007},
	Bdsk-Url-1 = {https://doi.org/10.1007/s10115-007-0114-2}}

@article{schapire,
	abstract = {This paper addresses the problem of improving the accuracy of an hypothesis output by a learning algorithm in the distribution-free (PAC) learning model. A concept class is learnable (or strongly learnable) if, given access to a source of examples of the unknown concept, the learner with high probability is able to output an hypothesis that is correct on all but an arbitrarily small fraction of the instances. The concept class is weakly learnable if the learner can produce an hypothesis that performs only slightly better than random guessing. In this paper, it is shown that these two notions of learnability are equivalent.A method is described for converting a weak learning algorithm into one that achieves arbitrarily high accuracy. This construction may have practical applications as a tool for efficiently converting a mediocre learning algorithm into one that performs extremely well. In addition, the construction has some interesting theoretical consequences, including a set of general upper bounds on the complexity of any strong learning algorithm as a function of the allowed error Ïµ.},
	address = {USA},
	author = {Schapire, Robert E.},
	date-added = {2020-11-28 23:00:25 -0500},
	date-modified = {2020-11-28 23:00:25 -0500},
	doi = {10.1023/A:1022648800760},
	issn = {0885-6125},
	issue_date = {Jun. 1990},
	journal = {Mach. Learn.},
	keywords = {learnability theory, polynomial-time identification, Machine learning, learning from examples, PAC learning},
	month = jul,
	number = {2},
	numpages = {31},
	pages = {197--227},
	publisher = {Kluwer Academic Publishers},
	title = {The Strength of Weak Learnability},
	url = {https://doi.org/10.1023/A:1022648800760},
	volume = {5},
	year = {1990},
	Bdsk-Url-1 = {https://doi.org/10.1023/A:1022648800760}}

@inproceedings{data_imbalance_overview,
	author = {V. S. {Spelmen} and R. {Porkodi}},
	booktitle = {2018 International Conference on Current Trends towards Converging Technologies (ICCTCT)},
	doi = {10.1109/ICCTCT.2018.8551020},
	pages = {1-11},
	title = {A Review on Handling Imbalanced Data},
	year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1109/ICCTCT.2018.8551020}}

@inproceedings{undersampling1,
	address = {Cham},
	author = {Dal Pozzolo, Andrea and Caelen, Olivier and Bontempi, Gianluca},
	booktitle = {Machine Learning and Knowledge Discovery in Databases},
	editor = {Appice, Annalisa and Rodrigues, Pedro Pereira and Santos Costa, V{\'\i}tor and Soares, Carlos and Gama, Jo{\~a}o and Jorge, Al{\'\i}pio},
	isbn = {978-3-319-23528-8},
	pages = {200--215},
	publisher = {Springer International Publishing},
	title = {When is Undersampling Effective in Unbalanced Classification Tasks?},
	year = {2015}}

@article{undersampling2,
	author = {Weiss, G. M. and Provost, F.},
	doi = {10.1613/jair.1199},
	journal = {Journal of Artificial Intelligence Research},
	pages = {315--354},
	title = {Learning When Training Data are Costly: The Effect of Class Distribution on Tree Induction},
	volume = {19},
	year = {2003},
	Bdsk-Url-1 = {https://doi.org/10.1613/jair.1199}}

@inproceedings{undersampling3,
	author = {Cui, Ying-Jin and Davis, S. and Cheng, Chao-Kun and Bai, Xue},
	doi = {10.1109/ICMLC.2004.1380382},
	isbn = {0-7803-8403-2},
	journal = {Proceedings of 2004 International Conference on Machine Learning and Cybernetics},
	month = {09},
	pages = {3444 - 3448 vol.6},
	title = {A study of sample size with neural network},
	volume = {6},
	year = {2004},
	Bdsk-Url-1 = {https://doi.org/10.1109/ICMLC.2004.1380382}}

@article{undersampling_generalization,
	author = {M. {Wasikowski} and X. {Chen}},
	doi = {10.1109/TKDE.2009.187},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	number = {10},
	pages = {1388-1400},
	title = {Combating the Small Sample Class Imbalance Problem Using Feature Selection},
	volume = {22},
	year = {2010},
	Bdsk-Url-1 = {https://doi.org/10.1109/TKDE.2009.187}}

@inbook{notes_chap2,
	author = {Bloch, Matthieu R},
	booktitle = {ECE 6254 - Statistical Machine Learning},
	pages = {1--8},
	place = {Atlanta, Georgia},
	publisher = {Georgia Institute of Technology},
	title = {Introduction to Supervised Learning Theory},
	year = {2020}}

@inproceedings{undersampling_posterior,
	author = {A. D. {Pozzolo} and O. {Caelen} and R. A. {Johnson} and G. {Bontempi}},
	booktitle = {2015 IEEE Symposium Series on Computational Intelligence},
	doi = {10.1109/SSCI.2015.33},
	pages = {159-166},
	title = {Calibrating Probability with Undersampling for Unbalanced Classification},
	year = {2015},
	Bdsk-Url-1 = {https://doi.org/10.1109/SSCI.2015.33}}

@book{sampling_bias,
	author = {Qui{\~n}onero-Candela Joaquin},
	place = {Cambridge, Mass},
	publisher = {MIT},
	title = {Dataset shift in machine learning},
	year = {2009}}

@article{clonning,
	author = {Gy{\"o}rgy Kov{\'a}cs},
	doi = {https://doi.org/10.1016/j.asoc.2019.105662},
	issn = {1568-4946},
	journal = {Applied Soft Computing},
	keywords = {Imbalanced learning, SMOTE, Minority oversampling, SMOTE variants},
	pages = {105662},
	title = {An empirical comparison and evaluation of minority oversampling techniques on a large number of imbalanced datasets},
	url = {http://www.sciencedirect.com/science/article/pii/S1568494619304429},
	volume = {83},
	year = {2019},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S1568494619304429},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.asoc.2019.105662}}

@misc{smote,
	author = {Fernandez, Alberto and Garcia, Salvador and Herrera, Francisco and Chawla, Nitesh V.},
	journal = {Journal of Artificial Intelligence Research},
	month = {Apr},
	title = {SMOTE for Learning from Imbalanced Data: Progress and Challenges, Marking the 15-year Anniversary},
	url = {https://jair.org/index.php/jair/article/view/11192},
	year = {2018},
	Bdsk-Url-1 = {https://jair.org/index.php/jair/article/view/11192}}

@article{ensembles_review,
	author = {M. {Galar} and A. {Fernandez} and E. {Barrenechea} and H. {Bustince} and F. {Herrera}},
	doi = {10.1109/TSMCC.2011.2161285},
	journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
	number = {4},
	pages = {463-484},
	title = {A Review on Ensembles for the Class Imbalance Problem: Bagging-, Boosting-, and Hybrid-Based Approaches},
	volume = {42},
	year = {2012},
	Bdsk-Url-1 = {https://doi.org/10.1109/TSMCC.2011.2161285}}
