{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Boosting Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoostingSVM:\n",
    "    def __init__(self, x=None,y=None, sigma_ini = 100, sigma_min = 1, sigma_step = 0.5):\n",
    "        \"\"\"\n",
    "        This is a custom implementation of the Boosting SVM algorithm. It is based \n",
    "        on the combination of Adaboost and RFB SVM weak learners. To create the model,\n",
    "        this class needs the following inputs:\n",
    "        \n",
    "        X_train: Training features. Size N x D\n",
    "        y_train: Training labels. Size N x 1\n",
    "\n",
    "        \"\"\"\n",
    "        self.sigma_ini = sigma_ini\n",
    "        self.sigma_step = sigma_step\n",
    "        self.sigma_min = sigma_min\n",
    "    \n",
    "    def fit(self,X_train,y_train):\n",
    "        sigma = self.sigma_ini \n",
    "        \n",
    "        # Initialize weights\n",
    "        number_samples = np.shape(X_train)[0]\n",
    "        weights = np.ones(number_samples)/number_samples\n",
    "        \n",
    "        # Define vectors to store weak predictors and significances of each iteration\n",
    "        self.weak_learners = [] #np.zeros(shape=self.number_iterations, dtype=object)\n",
    "        self.significance_vec = [] #np.zeros(shape=self.number_iterations)\n",
    "        \n",
    "        # Todo: Apply dimensionality reduction\n",
    "        \n",
    "        #for iterations in range(self.number_iterations):\n",
    "        while sigma > self.sigma_min:\n",
    "            print('Sigma: %.1f' % sigma)\n",
    "            #print('BoostSVM iteration: %d' % (iterations))\n",
    "            current_weights = weights\n",
    "            \n",
    "            # Create and save week learner for this iteration\n",
    "            weak_learner = SVC(kernel='rbf', gamma = 1/2/sigma**2) #SVC(max_iter=10,tol=5)\n",
    "            weak_learner_model = weak_learner.fit(X_train, y_train, sample_weight=current_weights)\n",
    "\n",
    "            # The new weak learner model is saved\n",
    "            self.weak_learners.append(weak_learner_model)\n",
    "            weak_learner_pred = weak_learner_model.predict(X_train)\n",
    "            \n",
    "            # Calculate error\n",
    "            error = np.sum(current_weights[np.where(weak_learner_pred != y_train)[0]]) \n",
    "            \n",
    "            if error > 0.5:\n",
    "                sigma = sigma - self.sigma_step\n",
    "            else:\n",
    "                # Significance of the weak learner model is calculated and saved\n",
    "                significance = 0.5*np.log((1-error)/error) \n",
    "                self.significance_vec.append(significance)\n",
    "\n",
    "                # Update weights for each sample\n",
    "                idx_incorrect = np.where(weak_learner_pred != y_train)[0]\n",
    "                idx_correct = np.where(weak_learner_pred == y_train)[0]\n",
    "                weights[idx_incorrect] = np.multiply(current_weights[idx_incorrect],np.exp(significance))\n",
    "                weights[idx_correct] = current_weights[idx_correct]*np.exp(-significance)\n",
    "\n",
    "                # Normalize weights\n",
    "                weights /= weights.sum()\n",
    "        \n",
    "    def predict(self,X_test):\n",
    "        model_preds = np.array([model.predict(X_test) for model in self.weak_learners])\n",
    "        y_test_pred = np.sign(np.dot(self.significance_vec, model_preds))\n",
    "        return y_test_pred.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve and prepare dataset\n",
    "data = pd.read_pickle(\"./data.pkl\")\n",
    "\n",
    "# split data into X and y\n",
    "X = data.loc[:, data.columns != 'bot']\n",
    "Y = data.loc[:, data.columns == 'bot'].to_numpy()\n",
    "\n",
    "# split data into train and test sets\n",
    "seed = 10\n",
    "test_size = 0.33\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)\n",
    "\n",
    "# Labels for this algortihm has to be either 1 or -1\n",
    "y_train = np.where(y_train < 0.5, -1, 1)\n",
    "y_test = np.where(y_test < 0.5, -1, 1)\n",
    "\n",
    "y_train = y_train.flatten()\n",
    "y_test = y_test.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VSC - OK\n",
      "Sigma: 100.0\n",
      "Sigma: 100.0\n",
      "Sigma: 100.0\n",
      "Sigma: 99.5\n",
      "Sigma: 99.0\n",
      "Sigma: 98.5\n",
      "Sigma: 98.0\n",
      "Sigma: 97.5\n",
      "Sigma: 97.0\n",
      "Sigma: 96.5\n",
      "Sigma: 96.0\n",
      "Sigma: 95.5\n",
      "Sigma: 95.0\n",
      "Sigma: 94.5\n",
      "Sigma: 94.0\n",
      "Sigma: 93.5\n",
      "Sigma: 93.0\n",
      "Sigma: 92.5\n",
      "Sigma: 92.0\n",
      "Sigma: 91.5\n",
      "Sigma: 91.0\n",
      "Sigma: 90.5\n",
      "Sigma: 90.0\n",
      "Sigma: 89.5\n",
      "Sigma: 89.0\n",
      "Sigma: 88.5\n",
      "Sigma: 88.0\n",
      "Sigma: 87.5\n",
      "Sigma: 87.0\n",
      "Sigma: 86.5\n",
      "Sigma: 86.0\n",
      "Sigma: 85.5\n",
      "Sigma: 85.0\n",
      "Sigma: 84.5\n",
      "Sigma: 84.0\n",
      "Sigma: 83.5\n",
      "Sigma: 83.0\n",
      "Sigma: 82.5\n",
      "Sigma: 82.0\n",
      "Sigma: 81.5\n",
      "Sigma: 81.0\n",
      "Sigma: 80.5\n",
      "Sigma: 80.0\n",
      "Sigma: 79.5\n",
      "Sigma: 79.0\n",
      "Sigma: 78.5\n",
      "Sigma: 78.0\n",
      "Sigma: 77.5\n",
      "Sigma: 77.0\n",
      "Sigma: 76.5\n",
      "Sigma: 76.0\n",
      "Sigma: 75.5\n",
      "Sigma: 75.0\n",
      "Sigma: 74.5\n",
      "Sigma: 74.0\n",
      "Sigma: 73.5\n",
      "Sigma: 73.0\n",
      "Sigma: 72.5\n",
      "Sigma: 72.0\n",
      "Sigma: 71.5\n",
      "Sigma: 71.0\n",
      "Sigma: 70.5\n",
      "Sigma: 70.0\n",
      "Sigma: 69.5\n",
      "Sigma: 69.0\n",
      "Sigma: 68.5\n",
      "Sigma: 68.0\n",
      "Sigma: 67.5\n",
      "Sigma: 67.0\n",
      "Sigma: 66.5\n",
      "Sigma: 66.0\n",
      "Sigma: 65.5\n",
      "Sigma: 65.0\n",
      "Sigma: 64.5\n",
      "Sigma: 64.0\n",
      "Sigma: 63.5\n",
      "Sigma: 63.0\n",
      "Sigma: 62.5\n",
      "Sigma: 62.0\n",
      "Sigma: 61.5\n",
      "Sigma: 61.0\n",
      "Sigma: 60.5\n",
      "Sigma: 60.0\n",
      "Sigma: 59.5\n",
      "Sigma: 59.0\n",
      "Sigma: 58.5\n",
      "Sigma: 58.0\n",
      "Sigma: 57.5\n",
      "Sigma: 57.0\n",
      "Sigma: 56.5\n",
      "Sigma: 56.0\n"
     ]
    }
   ],
   "source": [
    "# Create models\n",
    "#classifierSVM = SVC()\n",
    "classifierBoostSVM = BoostingSVM()\n",
    "#classifierAda = AdaBoostClassifier(n_estimators=100)\n",
    "\n",
    "#classifierSVM.fit(X_train, y_train)\n",
    "print('VSC - OK')\n",
    "classifierBoostSVM.fit(X_train, y_train)\n",
    "print('BoostSVM - OK')\n",
    "#classifierAda.fit(X_train, y_train)\n",
    "print('AdaBoost - OK')\n",
    "\n",
    "#SVMpredictions = classifierSVM.predict(X_test)\n",
    "BoostSVMpredictions =classifierBoostSVM.predict(X_test)\n",
    "#Adapredictions = classifierAda.predict(X_test)\n",
    "\n",
    "#print('Accuracy with default SVM:      %.4f' % (len(np.where(y_test==SVMpredictions)[0])/len(y_test)))\n",
    "print('Accuracy with our Boosting SVM: %.4f' % (len(np.where(y_test==BoostSVMpredictions)[0])/len(y_test)))\n",
    "#print('Accuracy with AdaBoost:         %.4f' % (len(np.where(y_test==Adapredictions)[0])/len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy with our Boosting SVM: %.4f' % (len(np.where(y_test==BoostSVMpredictions)[0])/len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Considering the option of dimension reduction to speed-up SVM boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will consider "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
