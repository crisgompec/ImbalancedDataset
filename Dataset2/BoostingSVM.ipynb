{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Boosting Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoostingSVM:\n",
    "    def __init__(self, x=None,y=None, sigma_ini = 50, sigma_min = 1, sigma_step = 0.5, n_iterations = 100):\n",
    "        \"\"\"\n",
    "        This is a custom implementation of the Boosting SVM algorithm. It is based \n",
    "        on the combination of Adaboost and RFB SVM weak learners. To create the model,\n",
    "        this class needs the following inputs:\n",
    "        \n",
    "        X_train: Training features. Size N x D\n",
    "        y_train: Training labels. Size N x 1\n",
    "\n",
    "        \"\"\"\n",
    "        self.sigma_ini = sigma_ini\n",
    "        self.sigma_step = sigma_step\n",
    "        self.sigma_min = sigma_min\n",
    "        self.number_iterations = n_iterations\n",
    "    \n",
    "    def fit(self,X_train,y_train):\n",
    "        sigma = self.sigma_ini \n",
    "        \n",
    "        # Initialize weights\n",
    "        number_samples = np.shape(X_train)[0]\n",
    "        weights = np.ones(number_samples)/number_samples\n",
    "        \n",
    "        # Define vectors to store weak predictors and significances of each iteration\n",
    "        self.weak_learners = [] #np.zeros(shape=self.number_iterations, dtype=object)\n",
    "        self.significance_vec = [] #np.zeros(shape=self.number_iterations)\n",
    "        self.error_debug = []\n",
    "        \n",
    "        # Todo: Apply dimensionality reduction\n",
    "        self.pca = PCA(n_components = 2).fit(X_train)\n",
    "        self.variance_explained = self.pca.explained_variance_ratio_\n",
    "        X_train = self.pca.fit_transform(X_train)\n",
    "        \n",
    "        #for iterations in range(self.number_iterations):\n",
    "        n = 0\n",
    "        while sigma > self.sigma_min and n<self.number_iterations:\n",
    "            n += 1\n",
    "            print('Sigma: %.1f' % sigma)\n",
    "            #print('BoostSVM iteration: %d' % (iterations))\n",
    "            current_weights = weights\n",
    "            \n",
    "            # Create and save week learner for this iteration\n",
    "            weak_learner = SVC(kernel='rbf', gamma = 1/2/sigma**2) #SVC(max_iter=10,tol=5)\n",
    "            #weak_learner_model = weak_learner.fit(X_train, y_train, sample_weight=current_weights)\n",
    "            weak_learner_model = weak_learner.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "            # The new weak learner model is saved\n",
    "            weak_learner_pred = weak_learner_model.predict(X_train)\n",
    "            #print(weak_learner_pred[0:100])\n",
    "            print(current_weights)\n",
    "            \n",
    "            # Calculate error\n",
    "            error = np.sum(current_weights[np.where(weak_learner_pred != y_train)[0]]) \n",
    "            self.error_debug.append(error)\n",
    "            \n",
    "            incorrect_pred = 0\n",
    "            \n",
    "            for item_index in range(number_samples):\n",
    "\n",
    "                if weak_learner_pred[item_index] != y_train[item_index]:\n",
    "                    incorrect_pred = incorrect_pred + 1\n",
    "            #print(\"Incorrect pred:\" + str(incorrect_pred))   \n",
    "            \n",
    "            if error > 0.5:\n",
    "                sigma = sigma - self.sigma_step\n",
    "            else:\n",
    "                # Significance of the weak learner model is calculated and saved\n",
    "                significance = 0.5*np.log((1-error)/error) \n",
    "                self.significance_vec.append(significance)\n",
    "                self.weak_learners.append(weak_learner_model)\n",
    "\n",
    "                # Update weights for each sample\n",
    "                idx_incorrect = np.where(weak_learner_pred != y_train)[0]\n",
    "                idx_correct = np.where(weak_learner_pred == y_train)[0]\n",
    "                weights[idx_incorrect] = np.multiply(current_weights[idx_incorrect],np.exp(significance))\n",
    "                weights[idx_correct] = current_weights[idx_correct]*np.exp(-significance)\n",
    "\n",
    "                # Normalize weights\n",
    "                weights /= weights.sum()\n",
    "        \n",
    "    def predict(self,X_test):\n",
    "        X_test_pca = self.pca.fit_transform(X_test)\n",
    "        model_preds = np.array([model.predict(X_test_pca) for model in self.weak_learners])\n",
    "        y_test_pred = np.sign(np.dot(self.significance_vec, model_preds))\n",
    "        return y_test_pred.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "#scaler = StandardScaler() # Scaling is important for SVM because euclidean distances are involved\n",
    "data = pd.read_pickle(\"./data_no_cathegorical.pkl\")\n",
    "\n",
    "X = data.values[:,:-1]\n",
    "Y = data.values[:,-1]\n",
    "\n",
    "X = data.values[:int(len(X)*0.1),:-1]\n",
    "Y = data.values[:int(len(Y)*0.1),-1]\n",
    "\n",
    "# Split data into train and test sets\n",
    "seed = 10\n",
    "test_size = 0.33\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)\n",
    "\n",
    "# Labels for this algortihm has to be either 1 or -1\n",
    "y_train = np.where(y_train < 0.5, -1, 1)\n",
    "y_test = np.where(y_test < 0.5, -1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigma: 1000.0\n",
      "[0.00050659 0.00050659 0.00050659 ... 0.00050659 0.00050659 0.00050659]\n",
      "Sigma: 1000.0\n",
      "[0.00031348 0.00031348 0.00131926 ... 0.00031348 0.00131926 0.00031348]\n",
      "Sigma: 1000.0\n",
      "[0.00031348 0.00031348 0.00131926 ... 0.00031348 0.00131926 0.00031348]\n",
      "Sigma: 1000.0\n",
      "[0.00031348 0.00031348 0.00131926 ... 0.00031348 0.00131926 0.00031348]\n",
      "Sigma: 999.5\n",
      "[0.00031348 0.00031348 0.00131926 ... 0.00031348 0.00131926 0.00031348]\n",
      "Sigma: 999.0\n",
      "[0.00031348 0.00031348 0.00131926 ... 0.00031348 0.00131926 0.00031348]\n",
      "Sigma: 998.5\n",
      "[0.00031348 0.00031348 0.00131926 ... 0.00031348 0.00131926 0.00031348]\n",
      "Sigma: 998.0\n",
      "[0.00031348 0.00031348 0.00131926 ... 0.00031348 0.00131926 0.00031348]\n",
      "Sigma: 997.5\n",
      "[0.00031348 0.00031348 0.00131926 ... 0.00031348 0.00131926 0.00031348]\n",
      "Sigma: 997.0\n",
      "[0.00031348 0.00031348 0.00131926 ... 0.00031348 0.00131926 0.00031348]\n",
      "Sigma: 996.5\n",
      "[0.00031348 0.00031348 0.00131926 ... 0.00031348 0.00131926 0.00031348]\n",
      "Sigma: 996.5\n",
      "[0.00031265 0.00031265 0.00132275 ... 0.00031265 0.00132275 0.00031265]\n",
      "Sigma: 996.5\n",
      "[0.00031265 0.00031265 0.00132275 ... 0.00031265 0.00132275 0.00031265]\n",
      "Sigma: 996.0\n",
      "[0.00031265 0.00031265 0.00132275 ... 0.00031265 0.00132275 0.00031265]\n",
      "Sigma: 995.5\n",
      "[0.00031265 0.00031265 0.00132275 ... 0.00031265 0.00132275 0.00031265]\n",
      "Sigma: 995.0\n",
      "[0.00031265 0.00031265 0.00132275 ... 0.00031265 0.00132275 0.00031265]\n",
      "Sigma: 994.5\n",
      "[0.00031265 0.00031265 0.00132275 ... 0.00031265 0.00132275 0.00031265]\n",
      "Sigma: 994.0\n",
      "[0.00031265 0.00031265 0.00132275 ... 0.00031265 0.00132275 0.00031265]\n",
      "Sigma: 993.5\n",
      "[0.00031265 0.00031265 0.00132275 ... 0.00031265 0.00132275 0.00031265]\n",
      "Sigma: 993.0\n",
      "[0.00031265 0.00031265 0.00132275 ... 0.00031265 0.00132275 0.00031265]\n",
      "Sigma: 992.5\n",
      "[0.00031265 0.00031265 0.00132275 ... 0.00031265 0.00132275 0.00031265]\n",
      "Sigma: 992.0\n",
      "[0.00031265 0.00031265 0.00132275 ... 0.00031265 0.00132275 0.00031265]\n",
      "Sigma: 991.5\n",
      "[0.00031265 0.00031265 0.00132275 ... 0.00031265 0.00132275 0.00031265]\n",
      "Sigma: 991.0\n",
      "[0.00031265 0.00031265 0.00132275 ... 0.00031265 0.00132275 0.00031265]\n",
      "Sigma: 990.5\n",
      "[0.00031265 0.00031265 0.00132275 ... 0.00031265 0.00132275 0.00031265]\n",
      "Sigma: 990.0\n",
      "[0.00031265 0.00031265 0.00132275 ... 0.00031265 0.00132275 0.00031265]\n",
      "Sigma: 989.5\n",
      "[0.00031265 0.00031265 0.00132275 ... 0.00031265 0.00132275 0.00031265]\n",
      "Sigma: 989.0\n",
      "[0.00031265 0.00031265 0.00132275 ... 0.00031265 0.00132275 0.00031265]\n",
      "Sigma: 988.5\n",
      "[0.00031265 0.00031265 0.00132275 ... 0.00031265 0.00132275 0.00031265]\n",
      "Sigma: 988.0\n",
      "[0.00031265 0.00031265 0.00132275 ... 0.00031265 0.00132275 0.00031265]\n",
      "Sigma: 987.5\n",
      "[0.00031265 0.00031265 0.00132275 ... 0.00031265 0.00132275 0.00031265]\n",
      "Sigma: 987.0\n",
      "[0.00031265 0.00031265 0.00132275 ... 0.00031265 0.00132275 0.00031265]\n",
      "Sigma: 986.5\n",
      "[0.00031265 0.00031265 0.00132275 ... 0.00031265 0.00132275 0.00031265]\n",
      "Sigma: 986.0\n",
      "[0.00031265 0.00031265 0.00132275 ... 0.00031265 0.00132275 0.00031265]\n",
      "Sigma: 985.5\n",
      "[0.00031265 0.00031265 0.00132275 ... 0.00031265 0.00132275 0.00031265]\n",
      "Sigma: 985.0\n",
      "[0.00031265 0.00031265 0.00132275 ... 0.00031265 0.00132275 0.00031265]\n",
      "Sigma: 984.5\n",
      "[0.00031265 0.00031265 0.00132275 ... 0.00031265 0.00132275 0.00031265]\n",
      "Sigma: 984.0\n",
      "[0.00031265 0.00031265 0.00132275 ... 0.00031265 0.00132275 0.00031265]\n",
      "Sigma: 983.5\n",
      "[0.00031265 0.00031265 0.00132275 ... 0.00031265 0.00132275 0.00031265]\n",
      "Sigma: 983.0\n",
      "[0.00031265 0.00031265 0.00132275 ... 0.00031265 0.00132275 0.00031265]\n",
      "Sigma: 982.5\n",
      "[0.00031265 0.00031265 0.00132275 ... 0.00031265 0.00132275 0.00031265]\n",
      "Sigma: 982.5\n",
      "[0.00031183 0.00031183 0.00132626 ... 0.00031183 0.00132626 0.00031183]\n",
      "Sigma: 982.5\n",
      "[0.00031183 0.00031183 0.00132626 ... 0.00031183 0.00132626 0.00031183]\n",
      "Sigma: 982.5\n",
      "[0.00031183 0.00031183 0.00132626 ... 0.00031183 0.00132626 0.00031183]\n",
      "Sigma: 982.5\n",
      "[0.00031183 0.00031183 0.00132626 ... 0.00031183 0.00132626 0.00031183]\n",
      "Sigma: 982.5\n",
      "[0.00031183 0.00031183 0.00132626 ... 0.00031183 0.00132626 0.00031183]\n",
      "Sigma: 982.5\n",
      "[0.00031183 0.00031183 0.00132626 ... 0.00031183 0.00132626 0.00031183]\n",
      "Sigma: 982.5\n",
      "[0.00031183 0.00031183 0.00132626 ... 0.00031183 0.00132626 0.00031183]\n",
      "Sigma: 982.5\n",
      "[0.00031183 0.00031183 0.00132626 ... 0.00031183 0.00132626 0.00031183]\n",
      "Sigma: 982.5\n",
      "[0.00031183 0.00031183 0.00132626 ... 0.00031183 0.00132626 0.00031183]\n",
      "Sigma: 982.5\n",
      "[0.00031183 0.00031183 0.00132626 ... 0.00031183 0.00132626 0.00031183]\n",
      "Sigma: 982.5\n",
      "[0.00031183 0.00031183 0.00132626 ... 0.00031183 0.00132626 0.00031183]\n",
      "Sigma: 982.5\n",
      "[0.00031183 0.00031183 0.00132626 ... 0.00031183 0.00132626 0.00031183]\n",
      "Sigma: 982.5\n",
      "[0.00031183 0.00031183 0.00132626 ... 0.00031183 0.00132626 0.00031183]\n",
      "Sigma: 982.5\n",
      "[0.00031183 0.00031183 0.00132626 ... 0.00031183 0.00132626 0.00031183]\n",
      "Sigma: 982.5\n",
      "[0.00031183 0.00031183 0.00132626 ... 0.00031183 0.00132626 0.00031183]\n",
      "Sigma: 982.5\n",
      "[0.00031183 0.00031183 0.00132626 ... 0.00031183 0.00132626 0.00031183]\n",
      "Sigma: 982.5\n",
      "[0.00031183 0.00031183 0.00132626 ... 0.00031183 0.00132626 0.00031183]\n",
      "Sigma: 982.5\n",
      "[0.00031183 0.00031183 0.00132626 ... 0.00031183 0.00132626 0.00031183]\n",
      "Sigma: 982.5\n",
      "[0.00031183 0.00031183 0.00132626 ... 0.00031183 0.00132626 0.00031183]\n",
      "Sigma: 982.5\n",
      "[0.00031183 0.00031183 0.00132626 ... 0.00031183 0.00132626 0.00031183]\n",
      "Sigma: 982.5\n",
      "[0.00031183 0.00031183 0.00132626 ... 0.00031183 0.00132626 0.00031183]\n",
      "Sigma: 982.5\n",
      "[0.00031183 0.00031183 0.00132626 ... 0.00031183 0.00132626 0.00031183]\n",
      "Sigma: 982.5\n",
      "[0.00031183 0.00031183 0.00132626 ... 0.00031183 0.00132626 0.00031183]\n",
      "Sigma: 982.5\n",
      "[0.00031183 0.00031183 0.00132626 ... 0.00031183 0.00132626 0.00031183]\n",
      "Sigma: 982.5\n",
      "[0.00031183 0.00031183 0.00132626 ... 0.00031183 0.00132626 0.00031183]\n",
      "Sigma: 982.5\n",
      "[0.00031183 0.00031183 0.00132626 ... 0.00031183 0.00132626 0.00031183]\n",
      "Sigma: 982.5\n",
      "[0.00031183 0.00031183 0.00132626 ... 0.00031183 0.00132626 0.00031183]\n",
      "Sigma: 982.5\n",
      "[0.00031183 0.00031183 0.00132626 ... 0.00031183 0.00132626 0.00031183]\n",
      "Sigma: 982.5\n",
      "[0.00031183 0.00031183 0.00132626 ... 0.00031183 0.00132626 0.00031183]\n",
      "Sigma: 982.5\n",
      "[0.00031183 0.00031183 0.00132626 ... 0.00031183 0.00132626 0.00031183]\n",
      "Sigma: 982.5\n",
      "[0.00031183 0.00031183 0.00132626 ... 0.00031183 0.00132626 0.00031183]\n",
      "Sigma: 982.5\n",
      "[0.00031183 0.00031183 0.00132626 ... 0.00031183 0.00132626 0.00031183]\n",
      "Sigma: 982.5\n",
      "[0.00031183 0.00031183 0.00132626 ... 0.00031183 0.00132626 0.00031183]\n",
      "Sigma: 982.5\n",
      "[0.00031183 0.00031183 0.00132626 ... 0.00031183 0.00132626 0.00031183]\n",
      "Sigma: 982.5\n",
      "[0.00031183 0.00031183 0.00132626 ... 0.00031183 0.00132626 0.00031183]\n",
      "Sigma: 982.5\n",
      "[0.00031183 0.00031183 0.00132626 ... 0.00031183 0.00132626 0.00031183]\n",
      "Sigma: 982.5\n",
      "[0.00031183 0.00031183 0.00132626 ... 0.00031183 0.00132626 0.00031183]\n",
      "Sigma: 982.5\n",
      "[0.00031183 0.00031183 0.00132626 ... 0.00031183 0.00132626 0.00031183]\n",
      "Sigma: 982.5\n",
      "[0.00031183 0.00031183 0.00132626 ... 0.00031183 0.00132626 0.00031183]\n",
      "Sigma: 982.5\n",
      "[0.00031183 0.00031183 0.00132626 ... 0.00031183 0.00132626 0.00031183]\n",
      "Sigma: 982.5\n",
      "[0.00031183 0.00031183 0.00132626 ... 0.00031183 0.00132626 0.00031183]\n",
      "Sigma: 982.5\n",
      "[0.00031183 0.00031183 0.00132626 ... 0.00031183 0.00132626 0.00031183]\n",
      "Sigma: 982.5\n",
      "[0.00031183 0.00031183 0.00132626 ... 0.00031183 0.00132626 0.00031183]\n",
      "Sigma: 982.5\n",
      "[0.00031183 0.00031183 0.00132626 ... 0.00031183 0.00132626 0.00031183]\n",
      "Sigma: 982.5\n",
      "[0.00031183 0.00031183 0.00132626 ... 0.00031183 0.00132626 0.00031183]\n",
      "Sigma: 982.5\n",
      "[0.00031183 0.00031183 0.00132626 ... 0.00031183 0.00132626 0.00031183]\n",
      "Sigma: 982.5\n",
      "[0.00031183 0.00031183 0.00132626 ... 0.00031183 0.00132626 0.00031183]\n",
      "Sigma: 982.5\n",
      "[0.00031183 0.00031183 0.00132626 ... 0.00031183 0.00132626 0.00031183]\n",
      "Sigma: 982.5\n",
      "[0.00031183 0.00031183 0.00132626 ... 0.00031183 0.00132626 0.00031183]\n",
      "Sigma: 982.5\n",
      "[0.00031183 0.00031183 0.00132626 ... 0.00031183 0.00132626 0.00031183]\n",
      "Sigma: 982.5\n",
      "[0.00031183 0.00031183 0.00132626 ... 0.00031183 0.00132626 0.00031183]\n",
      "Sigma: 982.5\n",
      "[0.00031183 0.00031183 0.00132626 ... 0.00031183 0.00132626 0.00031183]\n",
      "Sigma: 982.5\n",
      "[0.00031183 0.00031183 0.00132626 ... 0.00031183 0.00132626 0.00031183]\n",
      "Sigma: 982.5\n",
      "[0.00031183 0.00031183 0.00132626 ... 0.00031183 0.00132626 0.00031183]\n",
      "Sigma: 982.5\n",
      "[0.00031183 0.00031183 0.00132626 ... 0.00031183 0.00132626 0.00031183]\n",
      "Sigma: 982.5\n",
      "[0.00031183 0.00031183 0.00132626 ... 0.00031183 0.00132626 0.00031183]\n",
      "Sigma: 982.5\n",
      "[0.00031183 0.00031183 0.00132626 ... 0.00031183 0.00132626 0.00031183]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigma: 982.5\n",
      "[0.00031183 0.00031183 0.00132626 ... 0.00031183 0.00132626 0.00031183]\n",
      "Sigma: 982.5\n",
      "[0.00031183 0.00031183 0.00132626 ... 0.00031183 0.00132626 0.00031183]\n",
      "BoostSVM - OK\n",
      "Accuracy with our Boosting SVM: 0.7338\n"
     ]
    }
   ],
   "source": [
    "# Create models\n",
    "#classifierSVM = SVC()\n",
    "classifierBoostSVM = BoostingSVM(sigma_ini=1000)\n",
    "#classifierAda = AdaBoostClassifier(n_estimators=100)\n",
    "\n",
    "#classifierSVM.fit(X_train, y_train)\n",
    "#print('VSC - OK')\n",
    "classifierBoostSVM.fit(X_train, y_train)\n",
    "print('BoostSVM - OK')\n",
    "#classifierAda.fit(X_train, y_train)\n",
    "#print('AdaBoost - OK')\n",
    "\n",
    "#SVMpredictions = classifierSVM.predict(X_test)\n",
    "BoostSVMpredictions = classifierBoostSVM.predict(X_test)\n",
    "#Adapredictions = classifierAda.predict(X_test)\n",
    "\n",
    "#print('Accuracy with default SVM:      %.4f' % (len(np.where(y_test==SVMpredictions)[0])/len(y_test)))\n",
    "print('Accuracy with our Boosting SVM: %.4f' % (len(np.where(y_test==BoostSVMpredictions)[0])/len(y_test)))\n",
    "#print('Accuracy with AdaBoost:         %.4f' % (len(np.where(y_test==Adapredictions)[0])/len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -1,  1, -1, -1, -1, -1, -1, -1, -1, -1,  1, -1,  1, -1, -1,  1,\n",
       "       -1, -1, -1, -1, -1,  1, -1,  1, -1,  1, -1, -1, -1,  1,  1, -1, -1,\n",
       "       -1, -1, -1, -1,  1, -1,  1,  1, -1, -1,  1, -1, -1, -1, -1,  1, -1,\n",
       "       -1, -1, -1, -1, -1, -1,  1, -1, -1, -1,  1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1,  1,  1, -1, -1,  1, -1,  1, -1, -1,  1,  1, -1, -1,  1, -1,\n",
       "       -1, -1, -1,  1, -1, -1, -1,  1,  1, -1, -1, -1,  1, -1,  1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with our Boosting SVM: 0.7338\n"
     ]
    }
   ],
   "source": [
    "BoostSVMpredictions = classifierBoostSVM.predict(X_test)\n",
    "#Adapredictions = classifierAda.predict(X_test)\n",
    "\n",
    "#print('Accuracy with default SVM:      %.4f' % (len(np.where(y_test==SVMpredictions)[0])/len(y_test)))\n",
    "print('Accuracy with our Boosting SVM: %.4f' % (len(np.where(y_test==BoostSVMpredictions)[0])/len(y_test)))\n",
    "#print('Accuracy with AdaBoost:         %.4f' % (len(np.where(y_test==Adapredictions)[0])/len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_pca = classifierBoostSVM.pca.fit_transform(X_test)\n",
    "model_preds = np.array([model.predict(x_test_pca) for model in classifierBoostSVM.weak_learners])\n",
    "y_test_pred = np.sign(np.dot(classifierBoostSVM.significance_vec, model_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1919959473150963, 0.49999999999999983, 0.4999999999999999, 0.5000000000000003, 0.5000000000000003, 0.5000000000000003, 0.5000000000000003, 0.5000000000000003, 0.5000000000000003, 0.5000000000000003, 0.49868073878628005, 0.49999999999999983, 0.5000000000000001, 0.5000000000000001, 0.5000000000000001, 0.5000000000000001, 0.5000000000000001, 0.5000000000000001, 0.5000000000000001, 0.5000000000000001, 0.5000000000000001, 0.5000000000000001, 0.5000000000000001, 0.5000000000000001, 0.5000000000000001, 0.5000000000000001, 0.5000000000000001, 0.5000000000000001, 0.5000000000000001, 0.5000000000000001, 0.5000000000000001, 0.5000000000000001, 0.5000000000000001, 0.5000000000000001, 0.5000000000000001, 0.5000000000000001, 0.5000000000000001, 0.5000000000000001, 0.5000000000000001, 0.5000000000000001, 0.4986772486772488, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(classifierBoostSVM.error_debug)\n",
    "print(classifierBoostSVM.error_debug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Considering the option of dimension reduction to speed-up SVM boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_components' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-d280de4c1f0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# PCA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplained_variance_ratio_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpca\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'n_components' is not defined"
     ]
    }
   ],
   "source": [
    "# SVM is very slow as it is computationally expensive. \n",
    "# We will try to extract the most significant features to speed-up the algorithm\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components).fit(X_train)\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(pca)\n",
    "Xnew_pca= pca.fit_transform(X_train)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(Xnew_pca,y_train,'o')\n",
    "plt.title('PCA') \n",
    "\n",
    "# LDA\n",
    "#lda = LDA(n_components = 1)\n",
    "#Xnew_lda = lda.fit_transform(X_train, y_train)\n",
    "\n",
    "#plt.figure()\n",
    "#plt.plot(Xnew_lda,y_train.resha,'o')\n",
    "#plt.title('LDA')\n",
    "\n",
    "# We choose PCA!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xnew_lda.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
