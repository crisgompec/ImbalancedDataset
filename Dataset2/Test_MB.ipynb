{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miguel/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  data = yaml.load(f.read()) or {}\n",
      "/home/miguel/anaconda3/lib/python3.7/site-packages/distributed/config.py:20: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  defaults = yaml.load(f)\n"
     ]
    }
   ],
   "source": [
    "#Â Libraries\n",
    "from classifiers import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "# from sklearn.ensemble import AdaBoostClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "data = pd.read_pickle(\"./data_no_cathegorical.pkl\")\n",
    "\n",
    "X = data.values[:,:-1]\n",
    "scaler.fit(X)\n",
    "X = scaler.transform(X)\n",
    "Y = data.values[:,-1]\n",
    "\n",
    "# Split data into train and test sets\n",
    "seed = 10\n",
    "test_size = 0.33\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)\n",
    "\n",
    "# Labels for this algortihm has to be either 1 or -1\n",
    "y_train = np.where(y_train < 0.5, -1, 1)\n",
    "y_test = np.where(y_test < 0.5, -1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADABOOST\n",
    "AdaBoost_classifier = AdaboostClassifier(n_iterations=100)\n",
    "AdaBoost_classifier.fit(X_train,y_train)\n",
    "classifier_list.append(AdaBoost_classifier)\n",
    "\n",
    "# BAGGING\n",
    "bagging_classifier = Bagging_classifier(type_classifier=\"SVM\")\n",
    "bagging_classifier.fit(X_train,y_train)\n",
    "classifier_list.append(bagging_classifier)\n",
    "\n",
    "# RANDOM FOREST\n",
    "RandomForest_Classifier = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "RandomForest_Classifier.fit(X_train, y_train)\n",
    "classifier_list.append(RandomForest_Classifier)\n",
    "\n",
    "# XGBOOST\n",
    "XGBoost_classifier = XGBClassifier(n_estimators=100)\n",
    "XGBoost_classifier.fit(X_train,y_train)\n",
    "classifier_list.append(XGBoost_classifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADACOST\n",
    "AdaCost_classifier = AdaCost(n_iterations=100)\n",
    "AdaCost_classifier.fit(X_train,y_train,cost=0.4)\n",
    "classifier_list.append(AdaCost_classifier)\n",
    "\n",
    "# ADAMEC\n",
    "AdaMEC_classifier = AdaMEC(n_iterations=100)\n",
    "AdaMEC_classifier = AdaMEC_classifier.fit(X_train,y_train)\n",
    "classifier_list.append(AdaMEC_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_majority = X_train[y_train < 1] # human (non-bots) => label -1 : are majority\n",
    "X_train_minority = X_train[y_train > 0]\n",
    "\n",
    "N = np.shape(y_train)[0]\n",
    "pi_mi = len(X_train_minority)/np.shape(y_train)[0]\n",
    "\n",
    "# X_train_majority = X_train_majority.to_numpy()\n",
    "# X_train_minority = X_train_minority.to_numpy()\n",
    "\n",
    "# y_test = np.where(y_test < 0.5, -1, 1)\n",
    "\n",
    "y_train = y_train.flatten()\n",
    "y_test = y_test.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_undersampled_dataset(Beta):\n",
    "    X_train_majority_undersampled = X_train_majority[random.sample(range(1, int((1-pi_mi)*N)),int(Beta*(1-pi_mi)*N)), :]\n",
    "    X_train_under = np.r_[X_train_majority_undersampled,X_train_minority]\n",
    "    y_train_under = np.r_[[-1]*len(X_train_majority_undersampled), [1]*len(X_train_minority)]\n",
    "    \n",
    "    return X_train_under, y_train_under"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADABOOST\n",
    "X_train_under, y_train_under = get_undersampled_dataset(0.62)\n",
    "AdaBoost_classifier_under = AdaboostClassifier(n_iterations=100)\n",
    "AdaBoost_classifier_under.fit(X_train_under,y_train_under)\n",
    "classifier_list.append(AdaBoost_classifier_under)\n",
    "    \n",
    "# BAGGING\n",
    "X_train_under, y_train_under = get_undersampled_dataset(0.58)\n",
    "bagging_classifier_under = Bagging_classifier(type_classifier=\"SVM\")\n",
    "bagging_classifier_under.fit(X_train_under,y_train_under)\n",
    "classifier_list.append(bagging_classifier_under)\n",
    "\n",
    "# RANDOM FOREST\n",
    "X_train_under, y_train_under = get_undersampled_dataset(0.52)\n",
    "RandomForest_Classifier_under = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "RandomForest_Classifier_under.fit(X_train_under, y_train_under)\n",
    "classifier_list.append(RandomForest_Classifier_under)\n",
    "    \n",
    "# XGBOOST\n",
    "X_train_under, y_train_under = get_undersampled_dataset(0.7)\n",
    "XGBoost_classifier_under = XGBClassifier(n_estimators=100)\n",
    "XGBoost_classifier_under.fit(X_train_under,y_train_under)\n",
    "classifier_list.append(XGBoost_classifier_under)\n",
    "\n",
    "# ADACOST\n",
    "X_train_under, y_train_under = get_undersampled_dataset(0.52)\n",
    "AdaCost_classifier_under = AdaCost(n_iterations=100)\n",
    "AdaCost_classifier_under.fit(X_train_under,y_train_under,cost=-0.4)\n",
    "classifier_list.append(AdaCost_classifier_under)\n",
    "    \n",
    "# ADAMEC\n",
    "X_train_under, y_train_under = get_undersampled_dataset(0.65)\n",
    "AdaMEC_classifier_under = AdaMEC(n_iterations=100)\n",
    "AdaMEC_classifier_under = AdaMEC_classifier_under.fit(X_train_under,y_train_under)\n",
    "classifier_list.append(AdaMEC_classifier_under)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data_MB_numpy = np.array(np.r_[4,8,22,3,float(9/(3.7*365)),int(3.7*365),1, 1,1, np.zeros(9),1,np.zeros(38),1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['favorites_count', 'followers_count', 'friends_count', 'statuses_count',\n",
       "       'average_tweets_per_day', 'account_age_days', 'default_profile_True',\n",
       "       'default_profile_image_True', 'geo_enabled_True', 'lang_af', 'lang_ar',\n",
       "       'lang_bg', 'lang_ca', 'lang_cs', 'lang_cy', 'lang_da', 'lang_de',\n",
       "       'lang_el', 'lang_en', 'lang_es', 'lang_et', 'lang_fa', 'lang_fi',\n",
       "       'lang_fr', 'lang_he', 'lang_hi', 'lang_hr', 'lang_hu', 'lang_id',\n",
       "       'lang_it', 'lang_ja', 'lang_ko', 'lang_lt', 'lang_lv', 'lang_mk',\n",
       "       'lang_ne', 'lang_nl', 'lang_no', 'lang_pa', 'lang_pl', 'lang_pt',\n",
       "       'lang_ro', 'lang_ru', 'lang_sk', 'lang_sl', 'lang_so', 'lang_sq',\n",
       "       'lang_sv', 'lang_sw', 'lang_th', 'lang_tl', 'lang_tr', 'lang_uk',\n",
       "       'lang_ur', 'lang_vi', 'lang_zh-cn', 'lang_zh-tw', 'verified_True',\n",
       "       'account_type_bot'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "MB_consensus = []\n",
    "for classifier in classifier_list: \n",
    "    y_pred = classifier.predict(np.reshape(X_data_MB_numpy,(1,58)))\n",
    "    MB_consensus.append(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-1]),\n",
       " array([-1]),\n",
       " array([-1]),\n",
       " array([-1]),\n",
       " array([-1]),\n",
       " array([-1]),\n",
       " array([-1]),\n",
       " array([-1]),\n",
       " array([-1]),\n",
       " array([-1]),\n",
       " array([-1]),\n",
       " array([-1])]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MB_consensus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 1)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(MB_consensus)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
